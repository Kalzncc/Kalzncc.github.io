{"meta":{"title":"Kalzn","subtitle":"","description":"","author":"Kalzn","url":"http://Kalzncc.github.io","root":"/"},"pages":[],"posts":[{"title":"SMO算法流程","slug":"smo","date":"2022-11-19T12:42:37.000Z","updated":"2022-11-19T12:43:51.112Z","comments":true,"path":"2022/11/19/smo/","link":"","permalink":"http://kalzncc.github.io/2022/11/19/smo/","excerpt":"","text":"SMO 算法流程 python代码见github 问题简介 SMO(Sequential Minimal Optimization)用于解决支持向量机中的对偶问题的最优化求解过程，该问题为： 而此问题也满足KKT条件要求 流程 该问题是一种凸二次规划问题，但是如果当作一般情况处理，计算过于繁琐。好在我们可以利用该问题特殊情况，得以特殊处理以简化流程。 SMO算法的核心思想是利用这一条件，进行特殊处理。由于一次性确定所有的最优化取值是十分困难，所谓我们不妨每次只考虑变更两个变量，然后唯一确定剩下的变量为。这里为什么选择两个变量，每次只选择一个不应该更容易吗？这里我们要注意，我们是通过迭代的方式每次选取一组的值进行更改。鉴于条件，我们是无法对单一进行修改的，换句话说，如果我们更改了一个变量，则必须有另一个变量跟随发生改变以满足。 以下，为了表述方便，我们每次选择的变量定为，此时目标函数可以写成： 这里我们把与无关的常数项都简写为，因为这部分在接下来的求导过程中无用。 这里引入我们之前的条件,并设定 带入消去得 其中 我们需要对其最大化，这里进行求导,赋值0求极值 至此，问题似乎得以解决，我们似乎只需要通过该等式解出即可。但是，请再次注意，我们是通过迭代的方式每次选取一组进行优化的。而注意到变量，它的取值为：，其中其他的变量我们无法获悉。我们只知道在之前的迭代中确定的旧值。 所以，这里我们考虑如何调整的数值。即，如何通过旧值推定出新值。我们假定，在之前的迭代中已经确定了一个拟定分隔超平面 这里为上一次迭代中的旧的值。这里我们明确，在此轮迭代中，改变的只有，所以有 所以我们将带入 将其带入得 其中为误差函数 但此时，我们还没有考虑到条件： 由于,故，上式无非就四种情况 其中(2)(3)可以归为一种情况 其中可以归结为。 满足线性规划 在这里插入图片描述 在这种情况下，应满足 。定义 此外，(1)(4)可以归为另一种情况 在这里插入图片描述 在这种情况下，应满足。定义 所以最后更新 至此我们确定了得更新值，然后的值也随之推出。这里我们设 则有 这里我们需要明确一件事情，到目前为止，我们所作的事情就是求这个目标函数得极值，通过分析可以发现是一个二次多项式函数，而二次项的系数为。所以目前来说，上述结论仅在时成立，因为此时是个开口向下的二次函数，存在极值为最小值。这种情况实际上可以应对大部分情况。但是在一部分情况，此时函数极小值在定义域边界出现。当然，在算法的实际实现中，我们可以直接求出定义域的两端值和极值，然后取三者中的最小值即可。 接下来，我们将讨论偏置的值如何求出。根据KKT条件可得，即有 带入误差函数得 其中为旧的偏置值，将该式子代入替换上式的前两项 同理可以得出 而最终的要取两者的中间值，即 最后，我们来讨论，如何进行变量的选取。首先我们应该确定第一个变量，此时，我们变量样本集，选取第一个不满足KKT条件的样本。这里写作KKT条件为： 然后依照规则选取第二个变量，执行优化。当完成后，我们开始遍历非边界样例集（即满足的样例），同样选择第一个不满足KKT条件的变量，然后依照一定规则选择出第二个变量进行优化。完成后，我们再次选择整个样本集进行以上操作。总得来说，我们交替选择整个样本集和非边界样本集进行优化，直至整个样本集全部满足KKT条件。 关于选取第二个变量的规则，我们的原则是让尽可能大的发生变化，由于依赖所以当为正，则要尽量小，否则要尽量大。 有时按照上述的启发式选择第二个变量，不能够使得函数值有足够的下降，这时按下述步骤: &gt; 首先在非边界集上选择能够使函数值足够下降的样本作为第二个变量， 如果非边界集上没有，则在整个样本集上选择第二个变量， 如果整个样本集依然不存在，则重新选择第一个变量。 参考 1.https://blog.csdn.net/luoshixian099/article/details/51227754 2.https://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html 3.https://www.jianshu.com/p/0c433f6f4141 4.https://zhuanlan.zhihu.com/p/257866920 5.John Platt.Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines (https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/)","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://kalzncc.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"基于C++的朴素贝叶斯分类器","slug":"simple-bayes","date":"2022-11-19T12:41:26.000Z","updated":"2022-11-19T12:42:29.417Z","comments":true,"path":"2022/11/19/simple-bayes/","link":"","permalink":"http://kalzncc.github.io/2022/11/19/simple-bayes/","excerpt":"","text":"基于C++的朴素贝叶斯分类器 github链接 使用c++编写的朴素贝叶斯分类器，其中似然中的离散分量，以及先验概率使用拉普拉斯平滑，连续分量为正态分布。 警告，此代码仅为初学学习之用，请勿用作任何工程项目！ ## 一、跑起来 ### 方式一 使用vscode+cmake插件或者Clion打开目录。然后直接编译运行。 ### 方式二 1、确保安装cmake环境，没有请先装cmake。 2、在工程目录下键入： 1234mkdir buildcd buildcmake ..make 3、运行build目录下的程序Bayers_classifier程序 二、用起来 1、建立模型 123456789101112131415161718Simple_Bayes_Classifier::Info info;/** struct Info { int sample_num; // 样例数量 std::vector&lt;int&gt; header; // 样例格式， // 如当前分量为离散值则为样例可能取值的数量， // 如为连续值则填0， // 例如，现有样例格式为这样 : // x0 属于 {\"东\",\"南\",\"西\",\"北\"} // x1 属于 {\"左\",\"右\"} // x2 属于 {x|0&lt;x&lt;100} 为连续值 // 则 header={4, 2, 0} int class_num; // 分类数量 int sample_size; // 样例分量维度大小 } */Simple_Bayes_Classifier model(info); 2、读取文件，训练模型 1model.train(\"data/1.txt\"); // 文件格式为：每行一个样例，每个样例n个分量用空格隔开，最后为该样例所属分类 示例文件格式： 3、开始分类, 构造出一个待分类的样例，然后分类结果赋值到样例的belong_to字段 1234Sample s;s.add_parameter(x); s.add_parameter(y);model.classify(s);std::cout &lt;&lt; s.belong_to &lt;&lt; std::endl; ## 三、学起来 贝叶斯分类器的基石为Bayes公式： 若现在存在样例的向量为，而其所属分类为的概率为： 其中，我们把称为先验概率（prior），而则为似然（likelihood）而称为证据（evidence）。当分类器工作时，遵循，我们需要比较种分类，选择概率最大的分类。 而 所以我们可以忽略证据，针对每个待分类的样例，对每种分类计算先验概率和似然即可。 先验概率一般直接进行数量统计，即: 其中为训练集中。所属类别的样例集，而为全体训练集。 而计算较为困难的是似然，在朴素贝叶斯中，我们认为向量的所有分量的取值是独立的,此时有: 此时即可进行运算，这里如果为离散值，则可以直接进行统计： 其中是训练集中满足：所属类别为且分量为的集合。 而如果为连续值，则这里可以将其看成正态分布: 其中分别为所属类别为的训练集的分量的方差和均值。 至此我们解决了朴素贝叶斯分类器。 在有些时候，向量的分量不是独立的，一种常见的情况是所有分量满足多维正态分布。为了清晰设置 其中 这里我们将结果取对数 此时有决策函数： 为决策界，当归为类，否则归为。至此，我们讨论了贝叶斯分类器中，样例各分量满足多维正态分布的情况。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://kalzncc.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"使用C++实现的简单ANN（人工神经网络）","slug":"ml-ann","date":"2022-11-19T12:01:22.000Z","updated":"2022-11-19T12:41:55.178Z","comments":true,"path":"2022/11/19/ml-ann/","link":"","permalink":"http://kalzncc.github.io/2022/11/19/ml-ann/","excerpt":"","text":"使用C++实现的简单ANN（人工神经网络） github地址 使用C++实现的最简单的人工神经网络，包含梯度下降的反向传播算法（BP）。内有部分注释，适合初学学习。至于为什么不用python？还是觉得从最底层（矩阵运算）写比较能加深印象和对算法的理解。（绝对不是因为我不会写python） 警告，此代码仅为初学学习之用，请勿用作任何工程项目！ ## 一、跑起来 ### 方式一 使用vscode+cmake插件或者Clion打开目录。然后直接编译运行。 ### 方式二 1、确保安装cmake环境，没有请先装cmake。 2、在工程目录下键入： 1234mkdir buildcd buildcmake ..make 3、运行build目录下的ANN程序 然后在data目录下生成文件output.csv,这是一个回归函数的拟合。 拟合情况如下： 二、用起来 1、使用十分简便，首先新建ANN模型，设置误差函数cost及其对于输出层每一项的偏导，这里使用默认的平方差函数 123ANNModel model;model.cost = Sqrt_Cost_Func::sqrt_cost;model.d_cost = Sqrt_Cost_Func::d_sqrt_cost; 1、设置学习率（一般0.0001~0.1） 1model.learning_rate = 0.01; 2、开始添加层级，从输入层开始，直到输出层，这里请保证输入层的神经元个数与输入向量的维度相同。并设置这些层级的激活函数和其导数。 123456789101112131415161718// 输入层 1个神经元ANNLayer layer0(1);layer0.activition = Linear_Func::linear; // 设置本层激活函数为线性函数f(x)=x // 根据ANN结构，输入层的激活函数应设置为线性layer0.d_activition = Linear_Func::linear;// 设置本层激活函数的导数model.add_layer(layer0);// 隐藏层 20个神经元ANNLayer layer1(20);layer1.activition = Signmod_Func::signmod; // 设置本层激活函数为sigmodlayer1.d_activition = Signmod_Func::d_signmod;model.add_layer(layer1);// 输出层1个神经院ANNLayer layer2(1);layer2.activition = Linear_Func::linear;layer2.d_activition = Linear_Func::d_linear;model.add_layer(layer2); 3、编译模型 1Compiled_ANNModel compiled_model = model.compile(); 4、训练模型，查看输出 12Vector data, expectation;Vector output = compiled_model.feed(data, expectation); 5、只输出，不训练 1Vector output = compiled_model.get_output(data); 三、学起来 这里给出最终公式，公式的推导请见其他教程、参考书。 1、获得神经元的激活值，这里使用表示第层的第个神经元的激活值大小 其中 其中为第层的激活函数，为从层第个神经元链接到第层第个神经元的边权（注意下标的顺序！），另外是第层第个神经元的偏置阈值。为第层的神经元个数。 2、反向传播公式（以平方差误差函数为例） 其中 最终有 最后对、进行更新如下 其中，为第层激活函数的导数。为误差函数，为预期输出向量的分量。为学习率。 具体实现的解释请，见代码注释。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://kalzncc.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"关于我","slug":"about-me","date":"2022-11-19T11:30:11.000Z","updated":"2022-11-19T13:12:11.042Z","comments":true,"path":"2022/11/19/about-me/","link":"","permalink":"http://kalzncc.github.io/2022/11/19/about-me/","excerpt":"","text":"这里是Kalzn，打过ICPC但没拿过金，打过CCPC但也没拿过金，打过蓝桥但没拿过第一，上过大学但是双非，在中科院读研但不是计算(自动化、软件)所。 写博客主要是给自己看，顺带给阁下带来点帮助。 有关ICPC的东西应该不会更了(找工作的时候可能会再更？)，读研之后就不怎么接触了，目前主要更机器学习、项目代码。","categories":[],"tags":[]},{"title":"博客迁移说明，我是Kalzn","slug":"hello-world","date":"2022-11-19T09:52:43.465Z","updated":"2022-11-19T12:14:27.708Z","comments":true,"path":"2022/11/19/hello-world/","link":"","permalink":"http://kalzncc.github.io/2022/11/19/hello-world/","excerpt":"","text":"我懒，所以一直不想迁移博客，所以对CSDN广告的容忍还是很高的。 但是由于最近CSDN的广告真的是让我忍无可忍（开屏一个网页的超大广告是搞什么？）所以还是花点时间迁过来吧。之后CSDN还会更新，但是说不定那天就完全转移过来了捏。之前的博客我会慢慢搬（github找了一圈，也没找见有博客搬迁的脚本，有无大佬搞一搞。） CSDN链接","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://kalzncc.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]}